{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoritical Questions\n",
        "\n",
        "1.Can we use Bagging for regression problems.\n",
        "- Yes, Bagging works for both classification and regression.\n",
        "- It builds multiple regression models on bootstrap samples.\n",
        "- Final prediction is the average of all model outputs.\n",
        "- Helps reduce variance and improves stability.\n",
        "\n",
        "2. What is the difference between multiple model training and single model training\n",
        "- Single model training uses one model, more variance and risk of overfitting.\n",
        "- Multiple model training uses many models , predictions are combined for better stability.\n",
        "- Ensemble reduces errors compared to relying on one weak/unstable model.\n",
        "- More computationally expensive than training one model.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest\n",
        "- At each split, only a random subset of features is considered.\n",
        "- Helps reduce correlation between trees.\n",
        "- Increases model diversity and improves generalization.\n",
        "- Prevents a few strong features from dominating all trees.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score\n",
        "- Evaluation metric using samples not included in bootstrap training sets.\n",
        "- Acts like built-in cross-validation for Random Forest/Bagging.\n",
        "- Measures model accuracy using unused samples per tree.\n",
        "- Saves time by avoiding separate validation sets.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model\n",
        "- Calculate decrease in impurity (Gini/Entropy/MSE) when a feature splits data.\n",
        "- Higher impurity reduction → more important feature.\n",
        "- Can also use permutation importance by shuffling feature values.\n",
        "- Importance scores reflect contribution across all trees.\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier\n",
        "- Creates multiple bootstrap samples from the training data.\n",
        "- Trains a separate classifier on each sample.\n",
        "- Aggregates predictions using majority voting.\n",
        "- Reduces variance and improves stability.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifier’s performance\n",
        "- Use accuracy, precision, recall, F1-score on test data.\n",
        "- Check OOB score if available.\n",
        "- Use cross-validation for robust evaluation.\n",
        "\n",
        "8. How does a Bagging Regressor work\n",
        "- Trains multiple regression models on different bootstrap samples.\n",
        "- Each model makes a prediction.\n",
        "- Final output is the mean of all predictions.\n",
        "- Reduces prediction variance.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques\n",
        "- Improves accuracy and robustness.\n",
        "- Reduces variance, bias, or both depending on technique.\n",
        "- More stable predictions than single models and Works well with weak learners.\n",
        "\n",
        "10. What is the main challenge of ensemble methods\n",
        "- Higher computational cost and memory usage.\n",
        "- Harder to interpret compared to single models.\n",
        "- Training many models increases complexity.\n",
        "- Risk of overfitting if not tuned properly.\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques\n",
        "- Combining multiple models improves performance. Each model contributes differently to final prediction.\n",
        "- Diversity among models leads to error reduction.\n",
        "- A group of weak models can form a strong learner.\n",
        "\n",
        "12. What is a Random Forest Classifier\n",
        "- An ensemble of many decision trees using Bagging + feature randomness.\n",
        "- Trees vote to make the final class prediction.\n",
        "- Reduces overfitting compared to a single treeand Works well for large and high-dimensional datasets.\n",
        "\n",
        "13. What are the main types of ensemble techniques\n",
        "- Bagging ,Boosting ,Stacking, Voting/Blending methods.\n",
        "\n",
        "14. What is ensemble learning in machine learning\n",
        "- Technique that combines predictions of multiple models.\n",
        "- Helps improve accuracy, stability, and generalization , works by averaging, voting, or meta-learning.\n",
        "- Reduces weaknesses of individual models.\n",
        "\n",
        "15. When should we avoid using ensemble methods\n",
        "- When interpretability is crucial and dataset is very small\n",
        "- When computational resources are limited.\n",
        "- When a simple model already performs well.\n",
        "\n",
        "16. How does Bagging help in reducing overfitting\n",
        "- Uses multiple versions of the dataset (bootstrap samples).\n",
        "- Each model learns different data patterns.\n",
        "- Averaging predictions reduces variance.\n",
        "- Weakens the impact of noisy samples.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree\n",
        "- Reduces variance dramatically → less overfitting.\n",
        "- Uses feature randomness, improving generalization more robust to noise and outliers.\n",
        "- Produces higher accuracy in most cases.\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging\n",
        "- Creates diverse training sets by sampling with replacement.\n",
        "- Ensures each model sees a different subset of data.\n",
        "- Increases model diversity → reduces prediction variance.\n",
        "- Allows OOB evaluation using unused samples.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques\n",
        "- Fraud detection ,Customer churn prediction , Medical diagnosis and risk prediction , Recommendation systems and ranking tasks.\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "- Bagging reduces variance; Boosting reduces bias.\n",
        "- Bagging trains models independently; Boosting trains sequentially.\n",
        "- Bagging uses bootstrap samples; Boosting adjusts weights based on errors.\n",
        "- Boosting is more powerful but more prone to overfitting."
      ],
      "metadata": {
        "id": "LdqnDtHqY3Yz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHdS5jP8Y11Y"
      },
      "outputs": [],
      "source": [
        "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "base_dt = DecisionTreeClassifier(random_state=42)\n",
        "bag_clf = BaggingClassifier(estimator=base_dt,n_estimators=100,max_samples=0.8,bootstrap=True,random_state=42)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "base_dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "bag_reg = BaggingRegressor(estimator=base_dt_reg,n_estimators=100,max_samples=0.8,bootstrap=True,random_state=42)\n",
        "\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Bagging Regressor MSE:\", mse)\n"
      ],
      "metadata": {
        "id": "sOD1_LmlnmoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100,random_state=42)\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, imp in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name}: {imp:.4f}\")\n"
      ],
      "metadata": {
        "id": "N4g1NUtgn6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100,random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Decision Tree Regressor MSE:\", mse_dt)\n",
        "print(\"Random Forest Regressor MSE:\", mse_rf)\n"
      ],
      "metadata": {
        "id": "P2jicyCFoOkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=200,oob_score=True,bootstrap=True,random_state=42)\n",
        "\n",
        "rf_clf_oob.fit(X, y)\n",
        "\n",
        "print(\"OOB Score:\", rf_clf_oob.oob_score_)\n"
      ],
      "metadata": {
        "id": "g3DJG6acoeaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "base_svm = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "bag_svm = BaggingClassifier(\n",
        "    estimator=base_svm,\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_svm.fit(X_train, y_train)\n",
        "y_pred = bag_svm.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging (SVM) Accuracy:\", acc)\n"
      ],
      "metadata": {
        "id": "mdmIt0WMo8PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    rf_clf = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"n_estimators = {n}, Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "IvmI0gsHpLpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "base_log_reg = LogisticRegression(\n",
        "    max_iter=500,\n",
        "    solver='lbfgs'\n",
        ")\n",
        "\n",
        "bag_log_reg = BaggingClassifier(\n",
        "    estimator=base_log_reg,\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_proba = bag_log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"Bagging (Logistic Regression) AUC:\", auc)\n"
      ],
      "metadata": {
        "id": "3b3CgVWRpWVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train a Random Forest Regressor and analyze feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_reg.feature_importances_\n",
        "\n",
        "print(\"Random Forest Regressor Feature Importances:\")\n",
        "for name, imp in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name}: {imp:.4f}\")\n"
      ],
      "metadata": {
        "id": "UCWiTQznpkBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "base_dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    estimator=base_dt,\n",
        "    n_estimators=100,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", acc_bag)\n",
        "print(\"Random Forest Classifier Accuracy:\", acc_rf)\n"
      ],
      "metadata": {
        "id": "BWX6CFvApnzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}